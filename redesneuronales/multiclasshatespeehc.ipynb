{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T00:28:54.015754Z",
     "start_time": "2025-03-28T00:28:53.520949Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical #Acomodar y_train y y_test pra poder entrenar el modelo\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "stopwords_en = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "tqdm.pandas()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jorge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jorge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jorge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jorge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d5e8948fbce9350"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/1252/Procesamiento de Lenguaje Natural/Archivos/HateSpeech (1) (1).csv')\n",
    "df = df[['class', 'tweet']]\n",
    "df.head()"
   ],
   "id": "2d70f27fd26d1dad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "0: Hate Speech\n",
    "1: Offensive:\n",
    "2: Neither\n",
    "\"\"\"\n",
    "\n",
    "df['class'].unique()"
   ],
   "id": "53ed6611d109e289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#La desproporci√≥n puede alterar el modelo\n",
    "df['class'].value_counts().plot(kind='bar')"
   ],
   "id": "49313e73972ae570"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dafe66aa5851fbf6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "undersampled_data= pd.DataFrame(columns=['class', 'tweet'])\n",
    "undersampled_data = pd.concat([undersampled_data, df['class']], axis=0)\n",
    "undersampled_data = pd.concat([undersampled_data, df['class']], axis=0)"
   ],
   "id": "400eda231bd3a078"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nlp = spacy.load('en_core_web_lg')",
   "id": "7484007299fb9e5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def vectorize(texto):\n",
    "  texto = texto.lower()\n",
    "  texto = contractions.fix(texto)\n",
    "  texto = re.sub(r'@[A-Za-z0-9]+','', texto)\n",
    "  texto = re.sub(r'[^0-9A-Za-z \\t]','', texto)\n",
    "  texto = re.sub(r'\\w+:\\/\\/\\S+','', texto)\n",
    "  texto = re.sub(r'\\w+:\\/\\/\\S+','', texto)\n",
    "  texto = re.sub(r'rt','', texto)\n",
    "  texto = word_tokenize(texto)\n",
    "  texto = [token for token in texto if token not in stopwords_en]\n",
    "  texto = [lemmatizer.lemmatize(word) for word in texto]\n",
    "  texto = ' '.join(texto)\n",
    "  texto = nlp(texto).vector\n",
    "  return texto"
   ],
   "id": "9d9a630cb368aa80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['vector'] = df['tweet'].progress_apply(vectorize)\n",
    "df.head()"
   ],
   "id": "864dc0aa60deb5e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = df['vector']\n",
    "X = np.concatenate(X, axis=0).reshape(-1, 300)\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.7,\n",
    "                                                    random_state=101,\n",
    "                                                    stratify=y) #MANEJAR DESBALANCE ENTRE LA MULTICAPA\n",
    "\n"
   ],
   "id": "2c26d0edb9da6d9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_train = to_categorical(y_train,\n",
    "                         3) #Tenemos tres clases\n",
    "y_test = to_categorical(y_test,\n",
    "                         3) #Tenemos tres clases"
   ],
   "id": "b46932d21bc4a351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_train",
   "id": "55bf3b2ee48cd5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Sequential() #Para agregar capa por capa\n",
    "model.add(Dense(units = 128, activation = 'relu', input_dim = 300))\n",
    "model.add(Dropout(0.305))#el 35 se va a apagar de manera aleatoria\n",
    "\n",
    "model.add(Dense(units = 256, activation = 'relu'))\n",
    "model.add(Dropout(0.305))\n",
    "model.add(Dense(units = 3, activation = 'softmax')) #Usamos softmax al ser un problema no-binario\n",
    "model.add(Dropout(0.305))#el 35 se va a apagar de manera aleatoria\n",
    "model.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "df.reset_index(implace=True)"
   ],
   "id": "a96363460d015b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size = 10,\n",
    "                    epochs = 10,\n",
    "                    validation_data = (X_test, y_test),callbacks = [early_stopping])"
   ],
   "id": "8f3870d1cb7a2e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = pd.DataFrame(history.history)\n",
    "metrics"
   ],
   "id": "759e90d1b63e0848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sns.lineplot(data=metrics[['loss', 'val_loss']])",
   "id": "b540d264f6951ef0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sns.lineplot(data=metrics[['accuracy', 'val_accuracy']])",
   "id": "8b780c2699bc7035"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ],
   "id": "f7add2bceedef5c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = np.argmax(y_pred, axis = -1)\n",
    "y_pred"
   ],
   "id": "ccced7653d75f648"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.metrics import confusion_matrix, classification_report",
   "id": "e4cdf6603981bf1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(classification_report(y_test, y_pred))",
   "id": "fb81828ffc48be1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fat='2f')",
   "id": "165a159be03559f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "##cambios en la red neurona\n",
   "id": "4b6dbe4b9eb35af0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
