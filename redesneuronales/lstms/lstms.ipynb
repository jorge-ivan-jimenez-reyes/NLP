{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:12:30.822708Z",
     "start_time": "2025-04-03T23:12:30.817862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "e743530ef61c296",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T23:12:33.615153Z",
     "start_time": "2025-04-03T23:12:33.441190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('/Users/JorgeJimenez/Documents/UP/NLP/redesneuronales/lstms/canciones_eng_fin (1).xlsx')\n",
    "df = df[['class', 'tweet']]\n",
    "df.head()"
   ],
   "id": "fe06984a8bba20fc",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc6 in position 10: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mUnicodeDecodeError\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/Users/JorgeJimenez/Documents/UP/NLP/redesneuronales/lstms/canciones_eng_fin (1).xlsx\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m df = df[[\u001B[33m'\u001B[39m\u001B[33mclass\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mtweet\u001B[39m\u001B[33m'\u001B[39m]]\n\u001B[32m      3\u001B[39m df.head()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/NLP/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/NLP/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/NLP/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/NLP/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1895\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[32m   1897\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1898\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmapping\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1899\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1900\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/NLP/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001B[39m, in \u001B[36mCParserWrapper.__init__\u001B[39m\u001B[34m(self, src, **kwds)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m kwds[\u001B[33m\"\u001B[39m\u001B[33mdtype_backend\u001B[39m\u001B[33m\"\u001B[39m] == \u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m     91\u001B[39m     \u001B[38;5;66;03m# Fail here loudly instead of in cython after reading\u001B[39;00m\n\u001B[32m     92\u001B[39m     import_optional_dependency(\u001B[33m\"\u001B[39m\u001B[33mpyarrow\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28mself\u001B[39m._reader = \u001B[43mparsers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTextReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;28mself\u001B[39m.unnamed_cols = \u001B[38;5;28mself\u001B[39m._reader.unnamed_cols\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:574\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.__cinit__\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:663\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._get_header\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:874\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:891\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:2053\u001B[39m, in \u001B[36mpandas._libs.parsers.raise_parser_error\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:322\u001B[39m, in \u001B[36mdecode\u001B[39m\u001B[34m(self, input, final)\u001B[39m\n",
      "\u001B[31mUnicodeDecodeError\u001B[39m: 'utf-8' codec can't decode byte 0xc6 in position 10: invalid continuation byte"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow import truediv\n",
    "\n",
    "df['artist'].value_counts().plot(kind='bar')"
   ],
   "id": "bd61ad5171203de4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/1252/Procesamiento de Lenguaje Natural/Archivos/HateSpeech (1) (1).csv')\n",
    "df = df[['class', 'tweet']]\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "artistas = ['Ariana Grande', 'Iron Maiden', 'Ed Sheran']\n",
    "df = df[df['artista'].isin(artistas)]\n",
    "df.head"
   ],
   "id": "5ca217fd61a077f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['lyrics'.iloc[0].splitlines(keepends=true]",
   "id": "fe0cea135a8d1b6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = ''.join(df(['lyrics']).splitlines(keepends=True)\n",
    "data"
   ],
   "id": "8c60e73435f40902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as pd\n",
    "from tenssorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.optimizer import Adan\n"
   ],
   "id": "355a4af73f18969d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "##checar que use gpu yque se acompatible con tensorflow",
   "id": "9b5de40e46dd533b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ata)"
   ],
   "id": "562ad35a953792a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoded_text = tokenizer.texts_to_sequences(data)\n",
    "encoded_text\n"
   ],
   "id": "6e6ff9f2ae0d7425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocab_size = len(tokenizer.word_counts) +1\n",
    "vocab_size"
   ],
   "id": "ec4a9962fad4d674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "detalist = []\n",
    "for d in encoded_text:\n",
    "    if len(d)>1:\n",
    "        for i in range(2, len(d)):\n",
    "            detalist.append(d[i])\n",
    "\n",
    "    datalist 7"
   ],
   "id": "a30019b891b33605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d61f57613f04919c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
