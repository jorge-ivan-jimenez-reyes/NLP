{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-14T05:46:00.337324Z",
     "start_time": "2025-02-14T05:45:58.670438Z"
    }
   },
   "source": [
    "! pip install contractions\n",
    "! pip install punktab"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /opt/anaconda3/lib/python3.12/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /opt/anaconda3/lib/python3.12/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /opt/anaconda3/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in /opt/anaconda3/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement punktab (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for punktab\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:48:50.718302Z",
     "start_time": "2025-02-14T05:48:50.561452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams"
   ],
   "id": "d722fd36ef4ab6a1",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwordcloud\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WordCloud\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ngrams\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('/Users/jorgejimenez/Documents/UP/NLP/RegresionLogistica/deepsea.csv')\n",
    "df.head()"
   ],
   "id": "4aec39472249f07e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "f9f856c297365cfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "category_mapping = {'biology': 0, 'meme': 1}\n",
    "data_df['label'] = data_df['category'].map(category_mapping)\n",
    "data_df.head()"
   ],
   "id": "a1ef6318469c00e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_text(text):\n",
    "    # Arreglar contracciones\n",
    "    text = contractions.fix(text)\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar caracteres que no sean letras\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenizar\n",
    "    tokens = word_tokenize(text)\n",
    "    # Quitar stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lematizar cada token\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Reconstruir el texto\n",
    "    return ' '.join(tokens)\n"
   ],
   "id": "5e98b2226ecb806"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tqdm.pandas()\n",
    "data_df['clean_text'] = data_df['tweet'].progress_apply(process_text)\n",
    "data_df.head()\n"
   ],
   "id": "fa8969e4606ddc8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_ngrams(n, text_series):\n",
    "    ngram_list = []\n",
    "    for text in text_series:\n",
    "        # Generar tokens y sus n-gramas\n",
    "        tokens = word_tokenize(text)\n",
    "        n_grams = ngrams(tokens, n)\n",
    "        for gram in n_grams:\n",
    "            ngram_list.append(' '.join(gram))\n",
    "    return pd.Series(ngram_list).value_counts()\n"
   ],
   "id": "7ce2e79dc4a11bd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# N-gramas (bigrams) para todos los tweets\n",
    "n = 2\n",
    "bigram_freq = generate_ngrams(n, data_df['clean_text']).head(30)\n",
    "print(bigram_freq.head(10))\n"
   ],
   "id": "3775d29ce5fa87de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(bigram_freq.index, bigram_freq.values)\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2289ee6bf995de89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "wordcloud_all = WordCloud(width=2000, height=2000, colormap='Wistia_r',\n",
    "                          background_color='black', min_font_size=8)\n",
    "wordcloud_all = wordcloud_all.generate_from_frequencies(bigram_freq)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordcloud_all)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "b104f03c60cfee9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "bigram_meme = generate_ngrams(n, data_df[data_df['category'] == 'meme']['clean_text']).head(30)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(bigram_meme.index, bigram_meme.values)\n",
    "plt.xlabel('Frecuencia')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "9d3d94ba263ccd7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "wordcloud_meme = WordCloud(width=2000, height=2000, colormap='Wistia_r',\n",
    "                           background_color='black', min_font_size=8)\n",
    "wordcloud_meme = wordcloud_meme.generate_from_frequencies(bigram_meme)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(wordcloud_meme)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "cf6dbf994eca3045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Bigrams para tweets de la categoría \"biology\"\n",
    "bigram_biology = generate_ngrams(n, data_df[data_df['category'] == 'biology']['clean_text']).head(30)\n",
    "wordcloud_biology = WordCloud(width=2000, height=2000, colormap='Wistia_r',\n",
    "                              background_color='black', min_font_size=8)\n",
    "wordcloud_biology = wordcloud_biology.generate_from_frequencies(bigram_biology)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(wordcloud_biology)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "94510080e071f46c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data_df['clean_text'])\n",
    "tfidf_matrix\n"
   ],
   "id": "c7391a0939066c77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init='random')\n",
    "tsne_coords = tsne.fit_transform(tfidf_matrix)\n",
    "data_df['tsne_x'] = tsne_coords[:, 0]\n",
    "data_df['tsne_y'] = tsne_coords[:, 1]\n"
   ],
   "id": "3cd756481d95a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(nube)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "362c26b91f55e8e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # ngram with biology only\n",
    " n = 2\n",
    "cym_ngrams = generate_ngrams(n, df[df['category'] == 'biology']['clean_tweet']).head(30)\n",
    "cym_ngrams.head(10)"
   ],
   "id": "550b995f806422eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "nube=WordCloud(width=2000, height=2000, colormap='Wistia_r', background_color='black',min_font_size=8).generate_from_frequencies(cym_ngrams)",
   "id": "7f3970dc6bea16bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(nube)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c5194591810a05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "vectorized_tweets = tdidf_vectorizer.fit_transform(df['clean_tweet'])\n",
    "vectorized_tweets"
   ],
   "id": "7384216e4161a7f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, init='random')\n",
    "tsne_result = model.fit_transform(vectorized_tweets)\n",
    "tsne_result"
   ],
   "id": "878107e13000cb95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['tsne_1'] = tsne_result[:,0]\n",
    "df['tsne_2'] = tsne_result[:,1]"
   ],
   "id": "dc66d3f51ebf3fb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(\n",
    "    data_frame=df,\n",
    "    x=df['tsne_1'],\n",
    "    y=df['tsne_2'],\n",
    "    color=df['etiqueta'],\n",
    "    template='plotly_dark',\n",
    "    hover_data=['tweet']\n",
    ")\n",
    "fig.show()"
   ],
   "id": "cc34ea8cce4264c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['clean_tweet']\n",
    "Y = df['etiqueta']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, random_state=101)"
   ],
   "id": "5e8f3810773435f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "#Fit transform en datos de entrenamiento\n",
    "X_train_vectorized = tfidf.fit_transform(X_train)\n",
    "\n",
    "#Transform en datos de prueba\n",
    "X_test_vectorized = tfidf.transform(X_test)"
   ],
   "id": "48649cc324ae046e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, Y_train)"
   ],
   "id": "c1c3a20df267542d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred = model.predict(X_test_vectorized)",
   "id": "126298390a3939d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_pred",
   "id": "82c4370515893ce3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "Y_test",
   "id": "273be4bd4e13f04a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ],
   "id": "97bd1d2b94f153f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sns.heatmap(confusion_matrix(Y_test,y_pred), annot=True, fmt='.2f')",
   "id": "52ec7d6ee6427c08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(classification_report(Y_test,y_pred))\n",
    "# precision me importan los falsos positivos\n",
    "# recall me importan los falsos negativos\n",
    "# f1-score me importa el overall de todo"
   ],
   "id": "e5c86e38d5c2c580"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
