{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    " Instrucciones:\n",
    "\n",
    "Utilizarán el dataset de tweets sobre desastres naturales\n",
    "Estos tweets corresponden a 2 categorías. Los tweets marcados con un 1, corresponden a desastres reales, los marcados con 0 corresponden a desastres no reales\n",
    "\n",
    "1. Mapear los 1 a \"real disaster\", y los 0 a \"not-real disaster\"\n",
    "\n",
    "2. Crear nubes de palabras de unigramas. Una para los tweets sobre desastres verdaderos, y una para los tweets sobre desastres no reales\n",
    "\n",
    "3. Crear graficos de barras que muestren los 15 unigramas mas comunes. Uno para los tweets sobre desastres verdaderos, y uno para los tweets sobre desastres no reales\n",
    "\n",
    "4. Crear nubes de palabras de unigramas. Una para los tweets sobre desastres verdaderos, y una para los tweets sobre desastres no reales\n",
    "\n",
    "5. Crear graficos de barras que muestren los 15 unigramas mas comunes. Uno para los tweets sobre desastres verdaderos, y uno para los tweets sobre desastres no reales\n",
    "\n",
    "6. Crear nubes de palabras de bigramas. Una para los tweets sobre desastres verdaderos, y una para los tweets sobre desastres no reales\n",
    "\n",
    "7. Crear graficos de barras que muestren los 15 bigramas mas comunes. Uno para los tweets sobre desastres verdaderos, y uno para los tweets sobre desastres no reales\n",
    "\n",
    "8. Crear nubes de palabras de trigramas. Una para los tweets sobre desastres verdaderos, y una para los tweets sobre desastres no reales\n",
    "\n",
    "9. Crear graficos de barras que muestren los 15 trigramas mas comunes. Uno para los tweets sobre desastres verdaderos, y uno para los tweets sobre desastres no reales\n",
    "\n",
    "10. Crear una funcion de vectorizacion para los tweets del dataset. La funcion debera convertir a minusculas, eliminar hashtags, taggeos del tipo \"@usuario\" e hipervinculos. Eliminar stopwords, lematizar y obtener la representacion vectorial del texto con base en los vectores de GloVe (glove.6B.300d.txt). Aplicar dicha funcion sobre la columna que contiene el texto de los tweets. Guardar ese resultado en una columna nueva llamada \"vector\"\n",
    "\n",
    "11. Crear un modelo de TSNE que permita visualizar los vectores obtenidos en 2 dimensiones. El color estará dictado por la categoría del tweet (desastre real o no) y el hover data deberá ser el texto del tweet original\n",
    "\n",
    "12. Crear un pipeline de clasificacion para predecir el contenido de la columna \"target\". Pueden usar la cantidad de modelos e hiperparametros que quieran (nota: minimo dos modelos con dos combianaciones distitnas de parametros cada uno)\n",
    "\n",
    "13. Crear un motor de busqueda sobre el dataset utilizando similitud coseno (es decir, se debera vectorizar la pregunta  a buscar y computar su similitud contra todos los vectores del dataset. Al final, mostrar los resultados mas similares)\n",
    "Se sugiere implementar un parametro \"x\" para controlar el numero de records devueltos. Ejemplo: setear X a 10 implicaria regresar los 10 textos mas parecidos\n"
   ],
   "id": "6af6baefc775614f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T22:52:09.549116Z",
     "start_time": "2025-03-21T22:52:08.601991Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T22:58:22.253010Z",
     "start_time": "2025-03-21T22:58:22.225003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stopwords_en = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "11b5e1a644de7f0c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jorgejimenez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jorgejimenez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jorgejimenez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jorgejimenez/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jorgejimenez/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T22:58:26.751735Z",
     "start_time": "2025-03-21T22:58:26.720352Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('/Users/jorgejimenez/Documents/UP/NLP/redesneuronales/clickbait_dataset.csv', encoding = \"latin-1\")",
   "id": "13f8a033808005f9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T22:58:27.985240Z",
     "start_time": "2025-03-21T22:58:27.975588Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "ee29c6db7f473ed5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            headline  clickbait\n",
       "0  This Is What $1 USD Gets You In Food All Aroun...          1\n",
       "1  Make These Easy Chicken Fajita Quesadillas At ...          1\n",
       "2  The Hardest \"Walking Dead\" Video Game Quiz You...          1\n",
       "3  34 Online Shops Based In The Southeast You Sho...          1\n",
       "4  US and France to work together for new Iran sa...          0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Is What $1 USD Gets You In Food All Aroun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make These Easy Chicken Fajita Quesadillas At ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Hardest \"Walking Dead\" Video Game Quiz You...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34 Online Shops Based In The Southeast You Sho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US and France to work together for new Iran sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8edddc2cc681e348"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
