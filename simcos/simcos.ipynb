{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-06T22:59:11.959291Z",
     "start_time": "2025-03-06T22:57:22.042048Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "embeddings ={}\n",
    "#tomar archiv txt y contruir un diccionario\n",
    "with open(\"glove.42B.300d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:])\n",
    "        embeddings[word] = vectors\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T22:59:12.949764Z",
     "start_time": "2025-03-06T22:59:12.931780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = embeddings['dog'].astype('float')\n",
    "B = embeddings['dog'].astype('float')"
   ],
   "id": "e9dde57f503cbb52",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T22:59:13.993588Z",
     "start_time": "2025-03-06T22:59:13.906227Z"
    }
   },
   "cell_type": "code",
   "source": "A",
   "id": "6e0b4e4b3adf88e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3575e-01,  3.8897e-01, -4.1929e-01, -3.3219e-01,  5.3170e-01,\n",
       "       -2.5839e-01, -2.3869e+00, -4.3443e-01, -3.9760e-01, -9.9356e-01,\n",
       "        4.7093e-01, -1.6265e-01, -1.3474e-01, -1.3060e+00,  3.4694e-01,\n",
       "        1.2150e-01, -1.5811e-01, -1.1231e-02, -4.6560e-01, -1.8031e-01,\n",
       "        2.6682e-02, -2.8445e-02, -4.4228e-01,  2.0955e-01,  4.4307e-02,\n",
       "        2.7514e-01, -2.3140e-01, -1.0864e-01, -8.7113e-03,  2.0522e-01,\n",
       "        3.6109e-01, -3.5431e-01,  2.5217e-01,  2.6608e-01,  1.1942e-01,\n",
       "       -2.1606e-01,  7.3164e-02,  2.5023e-01,  2.4612e-01,  2.0797e-01,\n",
       "       -1.8702e-01, -3.8054e-02,  2.3604e-01,  4.2484e-01,  1.0187e-01,\n",
       "        5.8443e-02, -6.0782e-01, -5.2279e-01, -2.6276e-02, -1.4402e-01,\n",
       "        2.2169e-01, -1.5850e-01, -8.1178e-01,  8.2893e-02, -2.2136e-02,\n",
       "       -1.2966e-01,  1.7201e-01,  6.2484e-01, -2.3122e-02, -1.5704e-01,\n",
       "       -4.1946e-01, -4.9499e-01,  5.6224e-02, -8.1352e-02,  3.5428e-01,\n",
       "        1.5145e-01, -2.6535e-01,  1.0071e-01, -1.0047e+00,  3.4271e-01,\n",
       "       -3.0790e-03,  3.5994e-01,  4.0070e-01,  1.5180e-01,  1.1983e-01,\n",
       "       -3.0275e-01,  1.3739e-01, -3.6725e-01,  3.6650e-01,  3.1037e-01,\n",
       "        5.1300e-01,  2.0102e-01, -3.4841e-01,  2.8565e-01, -4.8071e-01,\n",
       "        2.1667e-01, -3.7125e-01,  6.0281e-01,  9.9829e-02, -4.7562e-01,\n",
       "       -9.4234e-02, -1.9625e-01, -1.2658e-01,  2.5421e-02, -1.0805e-01,\n",
       "       -9.5298e-01, -2.1365e+00, -2.5989e-01,  2.9010e-01, -1.4846e-01,\n",
       "       -6.3180e-02, -1.6572e-01,  7.3842e-03, -3.0222e-01, -2.8538e-01,\n",
       "        4.7556e-01,  4.1736e-02, -1.5516e-01,  3.9398e-01, -6.0727e-01,\n",
       "        7.8890e-01, -9.2844e-02,  3.2725e-01, -1.9599e-01, -2.1146e-01,\n",
       "        2.3927e-02, -1.7896e-01, -6.5021e-02,  5.2246e-01, -3.8243e-01,\n",
       "       -4.4670e-01,  7.9130e-02,  8.2286e-01,  5.5868e-01,  3.0237e-02,\n",
       "       -3.0010e-01,  4.8222e-01,  5.4899e-01, -3.2240e-01, -6.7086e-01,\n",
       "       -6.1622e-01,  6.6708e-02,  3.0572e-01,  1.5170e-02, -5.2623e-01,\n",
       "        1.5352e-01, -6.6473e-01, -2.0770e-01,  2.5136e-01,  3.6206e-01,\n",
       "       -2.2424e-01,  8.5694e-04,  4.6224e-02, -1.5066e-02, -3.2618e-01,\n",
       "       -9.7696e-02,  2.1966e-02,  2.7861e-01,  1.5840e-01, -2.0210e-01,\n",
       "        7.2365e-02, -1.9570e-01, -2.6862e-01,  1.3854e-01,  4.1905e-02,\n",
       "       -6.3822e-02, -3.2484e-01,  5.5546e-02,  2.8501e-01,  6.7519e-01,\n",
       "        3.0155e-01, -5.0474e-01, -6.7976e-01,  5.1983e-01,  6.5695e-02,\n",
       "        2.5286e-02, -9.6823e-02, -5.2386e-02, -5.7766e-02,  2.1080e-02,\n",
       "       -2.4691e-01, -8.0533e-01, -1.8712e-01,  4.7986e-01, -4.5478e-01,\n",
       "       -7.6232e-02,  6.3681e-01, -4.6249e-01,  2.3123e-01,  7.9812e-02,\n",
       "       -6.1783e-01, -5.4644e-02, -1.5799e-03,  1.4650e-01,  5.3362e-02,\n",
       "       -4.9770e-02, -6.3187e-01, -1.1563e-01,  3.8775e-01,  1.0124e-01,\n",
       "       -3.2881e-03,  1.7987e-01, -6.2480e-02, -2.7772e-01, -4.8288e-01,\n",
       "       -3.7275e-01,  6.1105e-01, -3.2270e-01,  6.9697e-01,  8.8196e-02,\n",
       "        4.4661e-01, -6.5324e-03, -1.0571e-01,  9.4600e-01, -1.9661e-01,\n",
       "       -3.9961e-01,  2.9346e-01,  1.3640e-01,  4.2281e-01, -4.5546e-01,\n",
       "        5.4213e-01, -1.9004e-01,  5.5959e-01, -3.9935e-01,  1.2659e-01,\n",
       "       -2.9526e-01, -1.1524e-01,  1.0132e-01, -1.6572e-01,  3.8628e-01,\n",
       "       -2.5172e-01,  3.0096e-01, -2.2338e-01, -9.3883e-02, -3.1209e+00,\n",
       "        7.9810e-02, -1.1115e+00,  1.6649e-01, -2.3107e-01, -1.2312e-01,\n",
       "        2.0972e-01, -7.0450e-03, -7.3021e-01, -4.7405e-01, -2.8147e-01,\n",
       "        1.0118e-03,  6.6102e-01, -1.1403e-02,  3.1912e-02,  5.0367e-01,\n",
       "        5.1864e-01, -6.0027e-02, -1.5285e-01,  7.3159e-01, -5.5330e-01,\n",
       "       -1.6912e-01, -6.3391e-02,  1.3858e-01,  2.8878e-01,  1.5076e-01,\n",
       "       -2.2552e-01,  2.8992e-01,  5.4796e-01,  1.5510e-01,  5.1603e-01,\n",
       "       -1.9686e-01, -6.4146e-01,  5.0649e-01,  7.0221e-01, -2.1417e-01,\n",
       "        3.5252e-01, -2.9347e-01, -1.9962e-01, -5.5284e-02, -1.0027e-01,\n",
       "        3.3330e-01,  4.9459e-01,  1.7876e-01, -4.1699e-02,  4.6649e-01,\n",
       "       -3.1720e-01, -3.2616e-01,  1.2663e-01, -4.3670e-01, -5.4299e-01,\n",
       "       -1.0077e-01, -2.6449e-01,  1.0524e-01, -3.2500e-01, -2.8024e-01,\n",
       "        4.9458e-01, -2.8948e-03, -8.6985e-01, -8.9116e-02, -2.3145e-01,\n",
       "       -4.0411e-01, -2.3680e-02,  2.4752e-01,  3.2651e-01, -5.3385e-02,\n",
       "       -1.1501e-01, -5.5804e-01,  4.8427e-01,  2.5167e-01, -6.8426e-02,\n",
       "        3.4227e-01,  2.6840e-01, -1.4929e-01, -2.3516e-01,  3.9194e-02])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T22:59:15.339575Z",
     "start_time": "2025-03-06T22:59:15.323176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "resultado = np.dot(A, B) /(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "resultado\n",
    "#similitud coseno con un vector es 100\n",
    "\n"
   ],
   "id": "bf16d9ff677d130b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T23:04:10.030572Z",
     "start_time": "2025-03-06T23:04:10.018781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "embeddings = {}  # Ensure this contains word embeddings as numpy arrays\n",
    "\n",
    "def getTopWords(word, n=10):\n",
    "    if word not in embeddings:\n",
    "        raise ValueError(f\"Word '{word}' not found in embeddings dictionary.\")\n",
    "\n",
    "    data = {\n",
    "        'words': [],\n",
    "        'sims': []\n",
    "    }\n",
    "\n",
    "    B = embeddings[word].astype('float')\n",
    "\n",
    "    for palabra in embeddings:\n",
    "        if palabra == word:\n",
    "            continue  # Skip comparing the word with itself\n",
    "\n",
    "        A = embeddings[palabra].astype('float')\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        resultado = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "        data['words'].append(palabra)\n",
    "        data['sims'].append(resultado)\n",
    "\n",
    "    # Create DataFrame and return top-n words\n",
    "    df = pd.DataFrame(data).sort_values(by=['sims'], ascending=False).head(n)\n",
    "    return df\n"
   ],
   "id": "b58b51d75a67d045",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T23:04:33.178916Z",
     "start_time": "2025-03-06T23:04:33.155202Z"
    }
   },
   "cell_type": "code",
   "source": "('watch')",
   "id": "8107cd17445f9463",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Word 'watch' not found in embeddings dictionary.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mgetTopWords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwatch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[34], line 8\u001B[0m, in \u001B[0;36mgetTopWords\u001B[0;34m(word, n)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgetTopWords\u001B[39m(word, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embeddings:\n\u001B[0;32m----> 8\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWord \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m not found in embeddings dictionary.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m     data \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msims\u001B[39m\u001B[38;5;124m'\u001B[39m: []\n\u001B[1;32m     13\u001B[0m     }\n\u001B[1;32m     15\u001B[0m     B \u001B[38;5;241m=\u001B[39m embeddings[word]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Word 'watch' not found in embeddings dictionary."
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# word analogies",
   "id": "32c3c575ca75e3f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T23:07:32.654439Z",
     "start_time": "2025-03-06T23:07:32.643495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculateAnalogy(posword1, posword2, reword1, n=10):\n",
    "    if posword1 not in embeddings or posword2 not in embeddings or reword1 not in embeddings:\n",
    "        raise ValueError(\"One or more words not found in embeddings.\")\n",
    "\n",
    "    # Convert word embeddings to NumPy float arrays\n",
    "    A = embeddings[posword1].astype('float')\n",
    "    B = embeddings[posword2].astype('float')\n",
    "    C = embeddings[reword1].astype('float')\n",
    "\n",
    "    # Calculate analogy vector: (posword1 - posword2 + reword1)\n",
    "    vector = A - B + C\n",
    "\n",
    "    data = {\n",
    "        'words': [],\n",
    "        'sims': []\n",
    "    }\n",
    "\n",
    "    for palabra in embeddings:\n",
    "        if palabra in {posword1, posword2, reword1}:  # Skip the input words\n",
    "            continue\n",
    "\n",
    "        word_vector = embeddings[palabra].astype('float')\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity = np.dot(word_vector, vector) / (np.linalg.norm(word_vector) * np.linalg.norm(vector))\n",
    "\n",
    "        data['words'].append(palabra)\n",
    "        data['sims'].append(similarity)\n",
    "\n",
    "    # Sort words by similarity and return top `n` matches\n",
    "    df = pd.DataFrame(data).sort_values(by=['sims'], ascending=False).head(n)\n",
    "    return df\n"
   ],
   "id": "cb80bea66eb91068",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T23:07:35.181447Z",
     "start_time": "2025-03-06T23:07:35.148891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "calculateAnalogy('mexico','spanish','dutch',20 )\n",
    "calculateAnalogy('mexico','spanish','dutch',20 )"
   ],
   "id": "dbc58df8fdba786d",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One or more words not found in embeddings.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcalculateAnalogy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmexico\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mspanish\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdutch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m calculateAnalogy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmexico\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspanish\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdutch\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;241m20\u001B[39m )\n",
      "Cell \u001B[0;32mIn[47], line 6\u001B[0m, in \u001B[0;36mcalculateAnalogy\u001B[0;34m(posword1, posword2, reword1, n)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculateAnalogy\u001B[39m(posword1, posword2, reword1, n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m posword1 \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embeddings \u001B[38;5;129;01mor\u001B[39;00m posword2 \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embeddings \u001B[38;5;129;01mor\u001B[39;00m reword1 \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m embeddings:\n\u001B[0;32m----> 6\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOne or more words not found in embeddings.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# Convert word embeddings to NumPy float arrays\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     A \u001B[38;5;241m=\u001B[39m embeddings[posword1]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: One or more words not found in embeddings."
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "s",
   "id": "a8347b6bc2020c80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
